{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Start building the deep learning model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(\"test.json\")\n",
    "df_train = pd.read_json(\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>italian</td>\n",
       "      <td>0</td>\n",
       "      <td>[penne, shallots, rice vinegar, fresh basil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greek</td>\n",
       "      <td>1</td>\n",
       "      <td>[sugar, chopped walnuts, filo dough, chopped a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italian</td>\n",
       "      <td>2</td>\n",
       "      <td>[fresh rosemary, chopped fresh thyme, fresh or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>3</td>\n",
       "      <td>[sugar, hot sauce, ramps, vinegar, cream chees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>french</td>\n",
       "      <td>4</td>\n",
       "      <td>[ground cinnamon, panettone, whipped cream, Am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine  id                                        ingredients\n",
       "0      italian   0  [penne, shallots, rice vinegar, fresh basil, g...\n",
       "1        greek   1  [sugar, chopped walnuts, filo dough, chopped a...\n",
       "2      italian   2  [fresh rosemary, chopped fresh thyme, fresh or...\n",
       "3  southern_us   3  [sugar, hot sauce, ramps, vinegar, cream chees...\n",
       "4       french   4  [ground cinnamon, panettone, whipped cream, Am..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29774, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuisine\n",
       "italian         5867\n",
       "mexican         4819\n",
       "southern_us     3234\n",
       "indian          2248\n",
       "chinese         2001\n",
       "french          1981\n",
       "cajun_creole    1157\n",
       "thai            1152\n",
       "japanese        1065\n",
       "greek            880\n",
       "spanish          740\n",
       "korean           621\n",
       "vietnamese       618\n",
       "moroccan         615\n",
       "british          602\n",
       "filipino         565\n",
       "irish            499\n",
       "jamaican         394\n",
       "russian          366\n",
       "brazilian        350\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisines = df_train.groupby([\"cuisine\"]).id.count()\n",
    "cuisines.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingredients = []\n",
    "for x in range(len(df_train.ingredients)):\n",
    "    ingredients.extend(df_train.ingredients[x])\n",
    "\n",
    "ingredients = np.array(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = np.unique(ingredients)\n",
    "ingredients.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(    oz.) tomato sauce', '(   oz.) tomato paste',\n",
       "       '(10 oz.) frozen chopped spinach', ..., 'ziti', 'zucchini',\n",
       "       'zucchini blossoms'], dtype='<U74')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_exceptions_to_remove = [\"\\xae\", \"\\xe8vre\", \"\\u2122\"]\n",
    "phrase_exceptions = [(\"7 up\", \"sevenup\"), (\"sun dried\", \"sundried\"), (\"bone less\", \"boneless\"), \n",
    "                     (\"skin less\", \"skinless\")]\n",
    "map_plural_to_singular = [(\"steaks\", \"steak\"), (\"loins\", \"loin\"), (\"inches\", \"inch\"), (\"centimeters\", \"centimeter\"),\n",
    "                          (\"ounces\", \"ounce\"), (\"liters\", \"liter\"), (\"mililiters\", \"mililiter\"), (\"grams\", \"gram\"),\n",
    "                          (\"cups\", \"cup\"), (\"gallons\", \"gallon\"), (\"quarts\", \"quart\"), (\"lbs\", \"lb\"),\n",
    "                          (\"pounds\", \"pound\"), (\"tablespoons\", \"tablespoon\"), (\"teaspoons\", \"teaspoon\"), \n",
    "                          (\"pints\", \"pint\"), (\"fluid ounces\", \"fluid ounce\"), (\"onions\", \"onion\"), \n",
    "                          (\"cloves\", \"clove\"), (\"bulbs\", \"bulb\"), (\"peppers\", \"pepper\"), (\"breasts\", \"breast\"),\n",
    "                          (\"eggs\", \"egg\"), (\"carrots\", \"carrot\"), (\"mushrooms\", \"mushroom\"),\n",
    "                          (\"tortillas\", \"tortilla\"), (\"sausages\", \"sausage\"), (\"wedges\", \"wedge\"), \n",
    "                          (\"tomatoes\", \"tomato\"), (\"thighs\", \"thigh\"), (\"chilies\", \"chili\"), (\"potatoes\", \"potato\"), \n",
    "                          (\"peppercorns\", \"peppercorn\"), (\"spices\", \"spice\"), (\"chiles\", \"chile\"), (\"apples\", \"apple\"),\n",
    "                          (\"legs\", \"leg\"), (\"doughs\", \"dough\"), (\"drumsticks\", \"drumstick\")]\n",
    "brandnames_to_remove = [\"alexia\", \"breakstones\", \"kraft\", \"bertolli classico\", \"bertolli\", \"best foods\", \n",
    "                        \"betty crocker\", \"bisquick\", \"bob evans\", \"breyers\", \"curry guy\", \"camellia\", \"campbells\", \n",
    "                        \"country crock\", \"crisco\", \"crystal farms\", \"delallo\", \"diamond crystal\", \"domino\", \n",
    "                        \"doritos\", \"earth balance\", \"egglands best\", \"foster farms\", \"franks\", \"gold medal\", \n",
    "                        \"goya\", \"green giant steamers niblets\", \"green giant\", \"heinz\", \"hellmanns\", \"herdez\", \n",
    "                        \"hidden valley\", \"honeysuckle white\", \"jacksonville\",  \"jimmy dean\", \"johnsonville\", \n",
    "                        \"knorr\", \"krudsen\", \"kikkoman\", \"lipton\", \"land o lakes\", \"mazola\", \"lea and perrins\", \n",
    "                        \"mccormick\", \"meyer\", \"mission\", \"old el paso\", \"old bay\", \"pam\", \"pepperidge farm\", \n",
    "                        \"oscar mayer\", \"pace\", \"pillsbury\", \"progresso\", \"pure wesson\", \"pompeian\", \"san marzano\", \n",
    "                        \"sargento\", \"soy vay\", \"taco bell\", \"yoplait\", \"spice islands\", \"stonefire\", \"success\", \n",
    "                        \"swanson\", \"truvía\", \"uncle bens\", \"wish bone\", \"zatarains\", \"morton\", \"jameson\", \"tapatio\", \n",
    "                        \"mountain high\", \"philadelphia\", \"king arthur\", \"roma\"]\n",
    "keywords_to_remove = [\"lowfat\", \"light\", \"shredded\", \"sliced\", \"all purpose\", \"all natural\", \"natural\", \"original\", \n",
    "                      \"gourmet\", \"traditional\", \"boneless\", \"skinless\", \"fresh\", \"nonfat\", \"pitted\", \"quick cooking\", \n",
    "                      \"unbleached\", \"part skim\", \"skim\", \"quickcooking\", \"oven ready\", \"homemade\", \"instant\", \"small\", \n",
    "                      \"extra large\", \"large\", \"chopped\", \"grated\", \"cooked\", \"stone ground\", \"freshly ground\", \n",
    "                      \"ground\", \"pure\", \"peeled\", \"deveined\", \"organic\", \"cracked\", \"granulated\", \"inch thick\", \n",
    "                      \"extra firm\", \"crushed\", \"flakes\", \"self rising\", \"diced\", \"crumbles\", \"crumbled\", \n",
    "                      \"whole wheat\", \"whole grain\", \"baby\", \"medium\", \"plain\", \"of\", \"thick cut\", \"cubed\", \"coarse\", \n",
    "                      \"free range\", \"seasoned\", \"canned\", \"multipurpose\", \"vegan\", \"thawed\", \"squeezed\", \n",
    "                      \"vegetarian\", \"fine\", \"zesty\", \"halves\", \"firmly packed\", \"drain\", \"drained\", \"washed\"]\n",
    "measurements_to_remove = [\"in\", \"inch\", \"cm\", \"centimeter\", \"oz\", \"ounce\", \"l\", \"liter\", \"ml\", \"mililiter\", \"g\", \n",
    "                          \"gram\", \"cup\", \"gallon\", \"quart\", \"lb\", \"pound\", \"tbsp\", \"tablespoon\", \"tsp\", \"teaspoon\", \n",
    "                          \"pint\", \"fl oz\", \"fluid ounce\"]\n",
    "phrases_to_remove = measurements_to_remove + keywords_to_remove + brandnames_to_remove\n",
    "phrases_to_map = [\n",
    "    ((\"green onion\", \"red onion\", \"purple onion\", \"yellow onion\", \"yel onion\"), \"onion\"),\n",
    "    ((\"collard green leaves\", \"collards\", \"collard leaves\"), \"collard greens\"),\n",
    "    (\"black pepper\", \"pepper\"),\n",
    "    (\"yel chives\", \"chives\"),\n",
    "    (\"spinach leaves\", \"spinach\"),\n",
    "    (\"tea leaves\", \"tea\"),\n",
    "    (\"chile\", \"chili\"),\n",
    "    ((\"garlic clove\", \"garlic bulb\"), \"garlic\"),\n",
    "    (\"uncooked\", \"raw\"),\n",
    "    ((\"red chili pepper\", \"hot chili pepper\", \"red hot chili pepper\"), \"chili pepper\"),\n",
    "    ((\"baking potato\", \"baked potato\"), \"baked potato\"),\n",
    "    ((\"sea salt\", \"kosher salt\", \"table salt\", \"white salt\"), \"salt\"),\n",
    "    (\"scotch whiskey\", \"scotch\"),\n",
    "    ((\"i cant believe its not butter spread\", \"i cant believe its not butter\"), \"butter\"),\n",
    "    ((\"extra virgin olive oil\", \"virgin olive oil\", \"mild olive oil\"), \"olive oil\"),\n",
    "    ((\"white bread\", \"wheat bread\", \"grain bread\"), \"bread\"),\n",
    "    ((\"white sugar\", \"yel sugar\"), \"sugar\"),\n",
    "    (\"confectioners sugar\", \"powdered sugar\"),\n",
    "    ((\"extra virgin coconut oil\", \"virgn coconut oil\"), \"coconut oil\")\n",
    "]\n",
    "\n",
    "# When executing multiple regex parses, it's most efficient to compile the expression ahead of time.\n",
    "punctuation_to_replace_with_space = re.compile(r\"[-,]\")\n",
    "percentage_less_to_remove = re.compile(r\"[\\d+]% less [A-z]*\")\n",
    "percentage_reduced_to_remove = re.compile(r\"[\\d+]% reduced [A-z]*\")\n",
    "symbols_to_remove = re.compile(r\"[!\\\\/%.'®™]\")\n",
    "digits_to_remove = re.compile(r\"\\d+\")\n",
    "symbols_to_replace_with_and = re.compile(r\"[&+]\")\n",
    "parentheses_content_to_remove = re.compile(r\"\\([^)]*\\)\")\n",
    "no_blank_added_to_remove = re.compile(r\"no [A-z]* added\")\n",
    "reduced_and_following_word_to_remove = re.compile(r\"reduced [A-z]*\")\n",
    "low_and_following_word_to_remove = re.compile(r\"low [A-z]*\")\n",
    "less_and_following_word_to_remove = re.compile(r\"less [A-z]*\")\n",
    "non_and_following_word_to_remove = re.compile(r\"non [A-z]*\")\n",
    "nonfat_removal = re.compile(r\"nonfat*\")\n",
    "nonhydrogenated_removal = re.compile(r\"nonhydrogenated*\")\n",
    "nondairy_removal = re.compile(r\"nondairy\")\n",
    "free_and_previous_word_to_remove = re.compile(r\"[A-z]* free\")\n",
    "multiple_spaces_to_trim = re.compile(r\" +\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise remove functions\n",
    "def remove_noise_ingredient(ingredient):\n",
    "     # Convert to lowercase.\n",
    "    ingredient = ingredient.lower()\n",
    "    \n",
    "    # Replace hyphens and commas with spaces.\n",
    "    ingredient = punctuation_to_replace_with_space.sub(\" \", ingredient)\n",
    "    \n",
    "    # Map certain exceptions that we don't want whiped out by later cleaning processes.\n",
    "    for character_exception in character_exceptions_to_remove:\n",
    "        ingredient = re.sub(character_exception, \"\", ingredient)\n",
    "    for phrase_exception, replacement in phrase_exceptions:\n",
    "        ingredient = re.sub(r\"\\b{}\\b\".format(phrase_exception), replacement, ingredient)\n",
    "        \n",
    "    # Remove \"percentage less\" instances (e.g., \"40% less sodium\") - do this before removing % and digits.\n",
    "    ingredient = percentage_less_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove \"percentage reduced\" instances (e.g., \"50% reduced fat\") - do this before removing % and digits.\n",
    "    ingredient = percentage_reduced_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove various unwanted symbols.\n",
    "    ingredient = symbols_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove digits.\n",
    "    ingredient = digits_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Replace common symbols with their word equivalent (so \"Ben&Jerry's\" == \"Ben and Jerry's\", etc.).\n",
    "    ingredient = symbols_to_replace_with_and.sub(\" and \", ingredient)\n",
    "    \n",
    "    # Remove anything between parentheses (this mainly includes volume measurements or \n",
    "    # unnecessary cooking instructions).\n",
    "    ingredient = parentheses_content_to_remove.sub(\" \", ingredient)\n",
    "    \n",
    "    # Remove all instances of \"no [sugar, sodium, fat, whatever] added\".\n",
    "    ingredient = no_blank_added_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove any instance of \"reduced\" and the word after it (e.g., 'reduced sodium').\n",
    "    ingredient = reduced_and_following_word_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove any instance of \"low\" and the word after it (e.g., 'low fat', 'low sodium').\n",
    "    ingredient = low_and_following_word_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove any instance of \"less\" and the word after it (e.g., 'less sodium').\n",
    "    ingredient = less_and_following_word_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove any instance of \"free\" and the word before it (e.g., 'sugar free', 'sodium free')\n",
    "    ingredient = free_and_previous_word_to_remove.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove any instance of \"non\" and the word after or \"nonfat\"/\"nonhydrogenated\"/\"nondairy\"\n",
    "    ingredient = non_and_following_word_to_remove.sub(\"\", ingredient)\n",
    "    ingredient = nonfat_removal.sub(\"\", ingredient)\n",
    "    ingredient = nonhydrogenated_removal.sub(\"\", ingredient)\n",
    "    ingredient = nondairy_removal.sub(\"\", ingredient)\n",
    "    \n",
    "    # Remove excess spacing in between words after first cleaning pass.\n",
    "    ingredient = multiple_spaces_to_trim.sub(\" \", ingredient)\n",
    "    \n",
    "    # Map common plural ingredients to singular ingredients\n",
    "    for plural, singular in map_plural_to_singular:\n",
    "        ingredient = re.sub(r\"\\b{}\\b\".format(plural), singular, ingredient)\n",
    "        \n",
    "    # Remove unuseful words\n",
    "    for phrase in phrases_to_remove:\n",
    "        ingredient = re.sub(r\"\\b{}\\b\".format(phrase), \"\", ingredient)\n",
    "    \n",
    "    # Map several similar phrases to the other equivalents to maintain consistency.\n",
    "    for pattern, replacement in phrases_to_map:\n",
    "        if ingredient in pattern:\n",
    "            ingredient = replacement\n",
    "    \n",
    "    # Remove excess spacing in between words after second cleaning pass and leading/trailing whitespace.\n",
    "    ingredient = multiple_spaces_to_trim.sub(\" \", ingredient)\n",
    "    ingredient = ingredient.strip()\n",
    "    \n",
    "    return ingredient\n",
    "\n",
    "\n",
    "def remove_noise_recipe(recipe):\n",
    "    recipe = list(map(remove_noise_ingredient, recipe))\n",
    "    \n",
    "    recipe = list(filter(lambda x: len(x) > 0, recipe))\n",
    "    \n",
    "    return recipe\n",
    "\n",
    "def remove_noise_data(raw_data):\n",
    "    clean_data = list(map(remove_noise_recipe, raw_data))\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning took %f 150.67625093460083\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_train.ingredients = remove_noise_data(df_train[\"ingredients\"])\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Cleaning took %f\", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this, it took forever.\n",
    "df_train.to_csv(\"df_train.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ingredients = []\n",
    "for li in df_train.ingredients:\n",
    "    cleaned_ingredients.extend(li)\n",
    "\n",
    "cleaned_ingredients = np.array(cleaned_ingredients)\n",
    "cleaned_ingredients = np.unique(cleaned_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a taste thai rice noodles', 'abalone', 'abbamele', ..., 'ziti',\n",
       "       'zucchini', 'zucchini blossoms'], dtype='<U63')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(df_train['ingredients'])\n",
    "train_encoded=t.texts_to_matrix(df_train['ingredients'],mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning took %f 50.260130405426025\n"
     ]
    }
   ],
   "source": [
    "#Clean test ingredients\n",
    "start_time = time.time()\n",
    "df_test.ingredients = remove_noise_data(df_test.ingredients)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Cleaning took %f\", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29774</td>\n",
       "      <td>[egg, beef stock, rice cakes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29775</td>\n",
       "      <td>[pasta, orange, thyme, peas, celery, tomato pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29776</td>\n",
       "      <td>[olive oil, onion, red wine, top sirloin steak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29777</td>\n",
       "      <td>[black pepper, patis, chicken stock, garlic, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29778</td>\n",
       "      <td>[pepper, garlic, tomato paste, salt, olive oil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients\n",
       "0  29774                      [egg, beef stock, rice cakes]\n",
       "1  29775  [pasta, orange, thyme, peas, celery, tomato pa...\n",
       "2  29776  [olive oil, onion, red wine, top sirloin steak...\n",
       "3  29777  [black pepper, patis, chicken stock, garlic, o...\n",
       "4  29778  [pepper, garlic, tomato paste, salt, olive oil..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5228)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded=t.texts_to_matrix(df_test['ingredients'],mode='tfidf')\n",
    "test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29774, 5228)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazilian': 0,\n",
       " 'british': 1,\n",
       " 'cajun_creole': 2,\n",
       " 'chinese': 3,\n",
       " 'filipino': 4,\n",
       " 'french': 5,\n",
       " 'greek': 6,\n",
       " 'indian': 7,\n",
       " 'irish': 8,\n",
       " 'italian': 9,\n",
       " 'jamaican': 10,\n",
       " 'japanese': 11,\n",
       " 'korean': 12,\n",
       " 'mexican': 13,\n",
       " 'moroccan': 14,\n",
       " 'russian': 15,\n",
       " 'southern_us': 16,\n",
       " 'spanish': 17,\n",
       " 'thai': 18,\n",
       " 'vietnamese': 19}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2index={cuisine:i for i,cuisine in enumerate(cuisines.index)}\n",
    "label2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for item in df_train.cuisine:\n",
    "    if item in label2index.keys():\n",
    "        y.append(label2index[item])\n",
    "y_encoded=to_categorical(y,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29774, 20)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_encoded,y_encoded,test_size=0.25,random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial_x_train, X_test, partial_y_train, y_test = train_test_split(X_train, y_train, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1000, activation='relu', input_shape=(5228,)))\n",
    "model.add(Dropout(0.75, name='dropout_1'))\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(5228,)))\n",
    "model.add(Dropout(0.8, name='dropout_2'))\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29774, 5228)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22330 samples, validate on 7444 samples\n",
      "Epoch 1/100\n",
      "22330/22330 [==============================] - 19s 849us/step - loss: 2.4742 - acc: 0.2994 - val_loss: 1.6745 - val_acc: 0.5676\n",
      "Epoch 2/100\n",
      "22330/22330 [==============================] - 18s 794us/step - loss: 1.7844 - acc: 0.5060 - val_loss: 1.3098 - val_acc: 0.6424\n",
      "Epoch 3/100\n",
      "22330/22330 [==============================] - 18s 793us/step - loss: 1.4706 - acc: 0.5815 - val_loss: 1.1147 - val_acc: 0.6847\n",
      "Epoch 4/100\n",
      "22330/22330 [==============================] - 17s 778us/step - loss: 1.2971 - acc: 0.6262 - val_loss: 1.0230 - val_acc: 0.7110\n",
      "Epoch 5/100\n",
      "22330/22330 [==============================] - 17s 771us/step - loss: 1.1918 - acc: 0.6565 - val_loss: 0.9658 - val_acc: 0.7274\n",
      "Epoch 6/100\n",
      "22330/22330 [==============================] - 17s 767us/step - loss: 1.0759 - acc: 0.6865 - val_loss: 0.9228 - val_acc: 0.7444\n",
      "Epoch 7/100\n",
      "22330/22330 [==============================] - 18s 788us/step - loss: 1.0056 - acc: 0.7044 - val_loss: 0.9021 - val_acc: 0.7505\n",
      "Epoch 8/100\n",
      "22330/22330 [==============================] - 18s 797us/step - loss: 0.9350 - acc: 0.7255 - val_loss: 0.8853 - val_acc: 0.7609\n",
      "Epoch 9/100\n",
      "22330/22330 [==============================] - 18s 799us/step - loss: 0.8801 - acc: 0.7386 - val_loss: 0.8743 - val_acc: 0.7641\n",
      "Epoch 10/100\n",
      "22330/22330 [==============================] - 18s 785us/step - loss: 0.8327 - acc: 0.7490 - val_loss: 0.8620 - val_acc: 0.7675\n",
      "Epoch 11/100\n",
      "22330/22330 [==============================] - 18s 793us/step - loss: 0.7945 - acc: 0.7631 - val_loss: 0.8676 - val_acc: 0.7724\n",
      "Epoch 12/100\n",
      "22330/22330 [==============================] - 18s 798us/step - loss: 0.7474 - acc: 0.7714 - val_loss: 0.8642 - val_acc: 0.7735\n",
      "Epoch 13/100\n",
      "22330/22330 [==============================] - 18s 799us/step - loss: 0.7200 - acc: 0.7794 - val_loss: 0.8620 - val_acc: 0.7761\n",
      "Epoch 14/100\n",
      "22330/22330 [==============================] - 18s 799us/step - loss: 0.6786 - acc: 0.7928 - val_loss: 0.8748 - val_acc: 0.7775\n",
      "Epoch 15/100\n",
      "22330/22330 [==============================] - 18s 785us/step - loss: 0.6687 - acc: 0.7968 - val_loss: 0.8813 - val_acc: 0.7773\n",
      "Epoch 16/100\n",
      "22330/22330 [==============================] - 18s 787us/step - loss: 0.6432 - acc: 0.8044 - val_loss: 0.8881 - val_acc: 0.7790\n",
      "Epoch 17/100\n",
      "22330/22330 [==============================] - 18s 792us/step - loss: 0.6102 - acc: 0.8121 - val_loss: 0.8961 - val_acc: 0.7806\n",
      "Epoch 18/100\n",
      "22330/22330 [==============================] - 17s 780us/step - loss: 0.5946 - acc: 0.8168 - val_loss: 0.9049 - val_acc: 0.7775\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262dedcf860>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor=[\n",
    "    EarlyStopping(monitor='val_loss',patience=5,verbose=1),\n",
    "    ModelCheckpoint('best-model-0.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "]\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "         validation_data=(X_val,y_val),\n",
    "         epochs=100,\n",
    "         callbacks=monitor,\n",
    "         batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_encoded)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 9, 9, 4, 9, 2, 9, 18, 3, 16]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_encoded = []\n",
    "for item in y_pred:\n",
    "    result_encoded.append(item.argmax())\n",
    "    \n",
    "result_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for i in result_encoded:\n",
    "    for k,v in label2index.items():\n",
    "        if v==i:\n",
    "            results.append(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['korean',\n",
       " 'italian',\n",
       " 'italian',\n",
       " 'filipino',\n",
       " 'italian',\n",
       " 'cajun_creole',\n",
       " 'italian',\n",
       " 'thai',\n",
       " 'chinese',\n",
       " 'southern_us']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(list(zip(df_test['id'],results)),columns=['id','cuisine'])\n",
    "submission.to_csv('submission.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29774</td>\n",
       "      <td>korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29775</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29776</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29777</td>\n",
       "      <td>filipino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29778</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   cuisine\n",
       "0  29774    korean\n",
       "1  29775   italian\n",
       "2  29776   italian\n",
       "3  29777  filipino\n",
       "4  29778   italian"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission=pd.read_csv('submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 2, 3, 18, 9, 9, 18, 16, 9, 13]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded = []\n",
    "for item in y_test:\n",
    "    y_test_encoded.append(item.argmax())\n",
    "    \n",
    "y_test_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_check = []\n",
    "\n",
    "for i in y_test_encoded:\n",
    "    for k,v in label2index.items():\n",
    "        if v==i:\n",
    "            y_test_check.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 2, 3, 19, 5, 5, 18, 16, 9, 9]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_encoded = []\n",
    "for item in y_pred_test:\n",
    "    y_pred_test_encoded.append(item.argmax())\n",
    "    \n",
    "y_pred_test_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mexican',\n",
       " 'cajun_creole',\n",
       " 'chinese',\n",
       " 'vietnamese',\n",
       " 'french',\n",
       " 'french',\n",
       " 'thai',\n",
       " 'southern_us',\n",
       " 'italian',\n",
       " 'italian']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_check = []\n",
    "\n",
    "for i in y_pred_test_encoded:\n",
    "    for k,v in label2index.items():\n",
    "        if v==i:\n",
    "            y_pred_test_check.append(k)\n",
    "\n",
    "y_pred_test_check[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7753895754970446"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_test_check, y_test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
